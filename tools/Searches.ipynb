{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from pdfminer.high_level import extract_text\n",
    "import codecs\n",
    "import sys\n",
    "from unidecode import unidecode\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "# Ordner mit PDFs und CSV-Datei für Suchwörter angeben\n",
    "pdf_folder = 'PDF/' #os.path.join(os.path.dirname(__file__), 'PDF/')\n",
    "search_terms_file = 'search_words.csv' #os.path.join(os.path.dirname(__file__), 'search_words.csv')\n",
    "output_file = 'results.csv' #os.path.join(os.path.dirname(__file__), 'results_fuzzy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        pdf_text = extract_text(f, codec='utf-8')\n",
    "    # Count the number of page breaks in the text to get the total number of pages\n",
    "    page_count = pdf_text.count('\\f')\n",
    "    return pdf_text, page_count\n",
    "\n",
    "\n",
    "\n",
    "with open(search_terms_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    search_words = [row[0] for row in reader]\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['PDF', 'Page', 'Search Word', 'Context'])\n",
    "\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_text, total_pages = read_pdf(pdf_folder + pdf_file)\n",
    "        for search_word in search_words:\n",
    "            escaped_word = re.escape(search_word)\n",
    "            # match_pattern = re.compile(search_word, re.IGNORECASE) # RegEx in CSV\n",
    "            match_pattern = re.compile(escaped_word, re.IGNORECASE)\n",
    "            for match in re.finditer(match_pattern, pdf_text):\n",
    "                start_index = max(0, match.start() - 100)\n",
    "                end_index = min(len(pdf_text), match.end() + 100)                \n",
    "                context = pdf_text[start_index:end_index]\n",
    "                context = re.sub(r'[^\\w\\s]+', '', context, flags=re.UNICODE).replace('\\n', ' ')\n",
    "                context = unidecode(context)\n",
    "                context = context.replace('\\f', '')\n",
    "                context = re.sub(r'\\s+', ' ', context)\n",
    "                page_number = pdf_text[:match.start()].count('\\f') + 1\n",
    "                with open(output_file, 'a', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([pdf_file, page_number, search_word, context])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suche abgeschlossen. Ergebnisse sind in CSV gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Funktion, die PDF-Seiten durchsucht und Übereinstimmungen mit Suchwörtern findet\n",
    "def search_pdf_pages(pdf_file_path, search_terms):\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        page_count = len(pdf_reader.pages)\n",
    "        matches = []\n",
    "        \n",
    "        for page_num in range(page_count):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text() if page.extract_text() else \"\"  # um sicherzustellen, dass page_text nicht None ist\n",
    "            \n",
    "            for term in search_terms:\n",
    "                max_similarity = 0\n",
    "                most_similar_word = \"\"\n",
    "\n",
    "                for word in page_text.split():\n",
    "                    if len(word) < 3:  # Filtern Sie kurze Wörter heraus\n",
    "                        continue\n",
    "                    current_similarity = fuzz.partial_ratio(word.lower(), term.lower())\n",
    "                    if current_similarity > max_similarity:\n",
    "                        max_similarity = current_similarity\n",
    "                        most_similar_word = word\n",
    "\n",
    "                if max_similarity >= 90:  # Oder ein anderer Schwellenwert nach Wahl\n",
    "                    start_index = page_text.lower().find(most_similar_word.lower())\n",
    "                    end_index = start_index + len(most_similar_word)\n",
    "\n",
    "                    # Extrahieren Sie den Kontext um das gefundene Wort\n",
    "                    context_start = max(0, start_index - 50)\n",
    "                    context_end = min(len(page_text), end_index + 50)\n",
    "                    context = page_text[context_start:context_end]\n",
    "\n",
    "                    # Entfernen von Zeilenumbrüchen, Tabulatoren und doppelten Leerzeichen\n",
    "                    context = re.sub(r'\\s+', ' ', context).strip()\n",
    "\n",
    "                    matches.append((pdf_file_path, page_num+1, term, max_similarity, most_similar_word, context))\n",
    "                    \n",
    "    return matches\n",
    "\n",
    "\n",
    "\n",
    "# Liste mit Suchwörtern aus CSV-Datei erstellen\n",
    "search_terms = []\n",
    "with open(search_terms_file, 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        search_terms.extend(row)\n",
    "\n",
    "# CSV-Datei für Ergebnisse öffnen und Spaltenüberschriften schreiben\n",
    "result_file = open(\"fuzzy_\" + output_file, 'w', newline='', encoding='utf-8')\n",
    "csvwriter = csv.writer(result_file)\n",
    "csvwriter.writerow(['PDF-Datei', 'Seite', 'Suchbegriff', 'Ähnlichkeit', 'Suchwort im Text'])\n",
    "\n",
    "# Durch alle PDF-Dateien im Ordner iterieren und Suchfunktion aufrufen\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_folder, filename)\n",
    "        matches = search_pdf_pages(file_path, search_terms)\n",
    "        for match in matches:\n",
    "            csvwriter.writerow([match[0], match[1], match[2], match[3], match[4], match[5]])\n",
    "\n",
    "            \n",
    "# CSV-Datei schließen\n",
    "result_file.close()\n",
    "\n",
    "print('Suche abgeschlossen. Ergebnisse sind in CSV gespeichert.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16c4443940534fb575232a06f051199749ae87166fcab54ddaf3350a00e3d73e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
